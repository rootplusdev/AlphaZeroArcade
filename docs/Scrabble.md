# Scrabble

This document sketches out a plan for tackling the game of Scrabble.

## Background

Our goal is to create the first superhuman (by 2024 standards) Scrabble AI.

Quackle is an open source program considered to be one of the strongest in the world. During the midgame, it works as follows:

- From any given board state and rack, every candidate move is enumerated, and that move’s _equity_ is computed.
Equity is `(A + B)`, where `A` is the move’s score value, and where `B` is an estimate of the value of the
move's _leave_ (Scrabble parlance for the tiles remaining on the rack after the move is played).
- For the top-k equity moves, a Monte Carlo simulation is performed, traversing a shallow depth down the game tree.
Each traversal samples the opponent’s rack uniformly randomly from the unseen tiles, and picks the top-equity move
for each player. Of the k tested moves, the one that performs best is chosen, measured by score-differential + equity-differential.

The leave-estimator is a simple lookup table for every possible leave, generated by some sort of offline simulation process.

Grandmaster Kenji Matsumoto has detailed the weaknesses of this approach on this [page](http://www.breakingthegame.net/computers5). Summarizing his observations:

- The AI has poor long-term planning, not willing to hold strong tiles for multiple turns.
- The uniform random assumption of the opponent’s rack is naive. In reality an opponent’s play gives us important information about their likely remaining tiles.
- Equity estimation is ignorant of board dynamics.
- Equity maximization fails to navigate the expectation-vs-variance tradeoff.

This excellent [video](https://youtu.be/oBmnpNwqE48?si=LG_PQzKs3VDRP1TW&t=276) by world-class Scrabble player Will Anderson
illustrates a situation where Quackle performs poorly, discussing in detail the AI mechanics described above. The
relevant section starts at 4:36 (link starts there) and ends at 7:04.

A principled, game-theoretically-sound approach, that converges towards the game’s Nash Equilibrium, should naturally overcome all these shortcomings.

In the endgame, Scrabble is a game of perfect information, like chess. As such, Quackle switches to a different mode, both
for the endgame, and for the pre-endgame. In theory, perfect-information routines like Alpha-beta pruning should work for the
endgame, but in practice, the game tree size is too large, and crafting a heuristic evaluation function to estimate leaf nodes of a partial
subtree is difficult. So Quackle uses B* search to sparsely explore the game tree. See this excellent blog 
[post](https://medium.com/@14domino/scrabble-is-nowhere-close-to-a-solved-game-6628ec9f5ab0) by Scrabble expert César Del Solar
for a technical discussion of empirical shortcomings of this approach.

For our planned AlphaZero-based implementation, the hope is that a single unified
algorithm can work across all phases of the game, with no need for endgame specialization. After all, "the game tree size is too large" and "crafting a heuristic evaluation
function is difficult" are exactly the reasons that Alpha-beta pruning failed at go, and AlphaZero was the answer there. If
this hope turns out be misplaced, we will later explore endgame specializations.

## Tree Search in Imperfect Information Games

A common objection to applying a tree-search algorithm to games of imperfect information
is that in imperfect information games, you cannot analyze a subtree of the game in isolation. 
This is because the value of a node in the game tree is dependent on the policy that got to that point in the tree.

Well...that’s only _partially_ true. It is true that the value of the node against the 
_worst case_ adversary policy is dependent on the policy that got to that point in the tree. This is because
given a policy `P`, the behavior of its optimal counter-policy, `counter(P)`,
at a node `N` is dependent on `P`'s behavior across the _entire_ game-tree, 
rather than just on `P`'s behavior in the subtree rooted at `N`.
However, against any given _fixed_ adversary policy, the value is independent of the behavior of `P`
outside of the subtree rooted at `N`.
So, if we hold the adversary policy fixed, a tree-search subroutine like MCTS could still act as a
policy improvement operator.

This begs the question: what good is policy improvement against some given _fixed_ policy? In rock-paper-scissor,
against a fixed 100%-rock opponent, iterated policy improvement will lead you to the 100%-paper strategy. That is
not the policy you want to converge to if you want to do well against other policies.

Despite this cautionary example, the fact is, other proven approaches like CFR and R-NaD operate similarly:
on any given generation, they assume a fixed adversary policy and perform a policy-improvement operator with
respect to that _fixed_ policy. Doing so yields a _sequence_ of policy-pairs, and as the policies in the pair
iteratively improve against each other, this _sequence_ (provably) converges towards equilibrium.

So, can MCTS, in the AlphaZero context, similarly serve as a policy-improvement operator that, despite
only representing an improvement against a fixed-opponent-policy _within_ a generation, represents a
long-term improvement towards equilibrium _across_ generations?

## Information Set MCTS (ISMCTS)

In perfect information settings, each node of an MCTS tree represents a full game state. In imperfect
information settings, the acting agent lacks visibility into all hidden information, and so cannot
construct such a tree.

[Blüml et al, 2023](https://www.frontiersin.org/articles/10.3389/frai.2023.1014561/full)
provides a comprehensive survey of various workarounds. Broadly, there are two approaches:

1. _Determinize_ the game by converting the imperfect information game into a perfect information one,
learn a policy for this game using perfect-information tree-search techniques, and then somehow convert the perfect-information strategy into
an imperfect-information one.

3. Construct an _information set_ MCTS (ISMCTS) tree, where each node represents an _information set_. This is the
part of the game state that is visible to the acting player. Devise tree search mechanics that operate
on this tree.

Of the second category of approaches, there is Many Tree Information Set MCTS (MT-ISMCTS),
introduced by [Cowling et al, 2015](http://orangehelicopter.com/academic/papers/cig15.pdf).
This serves as the starting point of our planned implementation. We wish to extend it to use AlphaZero
mechanics.

Obtaining a policy (P) and value (V) estimate at the root of the tree, when we are the current
acting player, is straightforward: train a neural network that takes as input the public game state together
with our private information. The big question is how to handle non-root nodes. Recursing at the next
level of the tree is not straightforward, because we are missing the private information of our opponent.
Without that, we cannot construct a proper input to pass to the network, and so cannot obtain a P and V estimate.

We thus need to instantiate the private information of our opponent. [Cowling et al, 2015](http://orangehelicopter.com/academic/papers/cig15.pdf)
describe a variety of approaches to sample this information, with accompanying experimental results. Their work predates AlphaGo/AlphaZero.
We will take the natural AlphaZero-inspired approach: train a _hidden-state_ neural network (H)
that learns to sample the hidden state of the game. Note that in principle, H can be computed exactly from P via
Bayes' Rule, but this computation can be expensive. So H can be considered an alternate representation of P that we
use for computational tractability.

In Scrabble, you can consider the opponent's entire rack as the hidden state, or you can consider just the leave. We choose to
use leaves rather than entire racks, as the training targets will be sharper without the diluting
effect of the uniform random bag replenishment.

H will accept the same inputs as P and V, and will be trained on self-play games using the actual leaves as training targets.

There are `>1e7` possible leaves in Scrabble, and outputting a logit distribution over all of them is likely unwieldy. 
Instead, we can have H sample the leave `t` tiles at a time, which only requires an output logit layer of size `27^t`.
We can sample the entire `T`-tile leave by performing `ceil(T/t)` queries to H, including the partially produced rack
as part of the input. In general games, an appropriate representation of the game's hidden state should be chosen
that allows for a tractable neural network output layer representation.

## Convergence to Equilibrium

If we play self-play games using MT-ISMCTS with `n` visits, and train P, V and H on the complete resultant set of 
self-play data, can we expect convergence to Nash Equilibrium, as `n` approaches infinity?

Formally, if we imagine the combined weights of the P, V, and H networks to be a point in `R^d`, then the generations
of training yields a path-like sequence of points in `R^d`: `x_1, x_2, ...`. There is some subset `NE` of `R^d`
representing the game's Nash equilibria. Does `x_i` have a limit, and if so, is that limit in `NE`?

Here is a soft-proof that the answer is yes.

The proof entails 3 parts:

1. **(Idempotence)**: If currently at equilibrium, we will stay there.
2. **(Non-Complacency)**: If not currently at equilibrium, we cannot stay there.
3. **(Limit-Existence)**: The limit of `x_i` must exist.

These 3 assertions clearly prove our desired result. Let us prove them in turn.

### Idempotence

Suppose that P, V, and H are at equilibrium.

The PUCT formula is:

```
PUCT(a) = Q(a) + c * P(a) * sqrt(sum(N)) / (1 + N(a))
```

The idempotence requirement means that as the number of iterations approaches infinity, the distribution of `N`
should approach the same shape as `P`. 

By standard properties of Nash equilibria, the assumption of equilibrium translates to the following: 
`P(a) == 0` for all `a` not in `S`, where `S` is the set of actions `a` for which `V(a)` attains it maximum
of `v_max`.

If `V` is accurate, as presumbed by the equilibrium hypothesis, then `Q(a)` at actions in `S` should converge
towards `v_max`, while `Q(a)` at actions not in `S` should converge towards values less than `v_max`. Since
`Q` dominates the equation as `N(a)` approach infinity, the proportion of `N` on actions not in `S`
should approach 0. It remains to show that the ratio `N(a_i) / N(a_j)` approaches `P(a_i) / P(a_j)`
for all `a_i, a_j` in `S`.

To see this, we can plug `a_i` and `a_j` into the `PUCT` equation, and set PUCT values equal. The `Q(a)`, 
`c`, and `sqrt(sum(N))` terms all cancel, leaving us with:

```
P(a_i) / P(a_j) = (1 + N(a_i)) / (1 + N(a_j))
```

This is enough. We can make the convergence better by replacing the `(1 + N(a))` term in the PUCT formula with
`max(c, N(a))` for some fixed constant `0 < c < 1`.

Some practicalities should be noted. Root softmax temperature must be disabled for this proof to work.
Dirichlet noise also breaks the analysis. Finally, move-temperature has the potential to make the agent
act with non-equilibrium frequencies. It is unclear if this last point will cause theoretical problems during
self-play, but certainly during competitive play, failure to act with equilibrium frequencies will lead to an
exploitable agent. Later, we will investigate how to deal with these issues practically.

### Non-Complacency

Suppose P, V and H are not at equilibrium. Then, there are game states at which they produce non-equilibrium estimates
(in other words, where P/V/H produce misestimates). Of these non-equilibrium states, choose a node `n` that is closest to
a terminal game state. This means that `n` is misestimated while all descendants of `n` are correctly estimated.

The networks cannot converge to this state, since `MT-ISMCTS` will produce a policy that exploits the
misestimates at `n`, and this exploitation corresponds to a difference between `P` and `N` that will cause network drift.

### Limit-Existence

If the limit does not exist, then the `x_i` necessarily follow a cyclical path. If we use the entire self-play
dataset as the experience buffer, however, an infinite cyclical path is not possible; we must eventually fall into the path's interior.

(Using the entire self-play dataset as the experience buffer is neither practical not desirable; we consider that
here purely for theoretical arguments.)

(This part of the psuedo-proof is questionable.)

## Details

### Action Representation

The number of possible moves in Scrabble can be large. One natural candidate move representation is of size 15 * 15 * 2 * 7! > 2e7.
The move location is encoded by the (15, 15), the direction (horizontal or vertical) by the 2, and the ordering of tiles
by the 7! = 840. This omits the letter-instantiation of the blank tiles, which technically can add another factor of 26^2.

Besides this number being too large to reasonably use for an output logit layer, asking the network to learn to encode
size-7 permutations is a tall task.

We can instead adopt an approach that has been proven to work in other high-branching-factor games like Arimaa: decompose the
move into _submoves_, and reformulate the game rules so that the current player takes multiple turns in a row. In Scrabble,
we can for example decompose a real move into multiple submoves:

1. Choose a board location + direction (15, 15, 2)
2. Choose tile 1 (53)
3. Choose tile 2 (53)
4. ...

The 53 here represents the 26 standard tiles and the 26 possible ways to play a blank tile, plus an extra 1 to indicate
a null-terminator. This reduces the output layer size to a much more manageable size, while also eliminating the need for the network to
learn permutation encodings.

(We omit pass/exchange moves here for simplicity of exposition.)

However, this decomposition has an undesirable property. To illustrate, consider holding a rack of `AEFINTS` in the starting
position of the game. The bingo `8D FAINEST` is the best play. But this shares the same tree-prefix as the move `8D FATES`
(specifically, they share the location/direction of `8D` and the first two tiles of `FA`). So the MCTS backpropagation step
for the move `8D FAINEST` ends up incentivizing the move `8D FATES`, when the two moves don't have anything to do with each
other.

More broadly, a good move decomposition scheme ideally has the property that if a move is good, then other moves that
share a decomposed-prefix are also likely to be good.

We can improve the decomposition scheme by instead making the number of tiles in the move part of submove 1:

1. Choose a board location + direction **+ move-size** (15, 15, 2, 7)
2. Choose tile 1 (52)
3. Choose tile 2 (52)
4. ...

In this scheme, `8D FAINEST` and `8D FATES` no longer share a decomposed-prefix.

We can also consider merging tile-selections to be two-at-a-time, to decrease the number of submoves. 52^2 is still 
completely manageable, and doing this merge reduces the number of network evaluations that would be needed.

There may be other decomposition schemes that are better than this one. For example, the **point-value** of the
play may serve a similar purpose as move-size, but generalize better. Experimentation is needed.

### Network Requerying Optimization

Repeatedly querying the network to generate a move in chunks, or to sample the hidden information in chunks, makes MCTS substantially
costlier. It may be possible to mitigate this by decomposing the network into two subnetworks.
The first, costlier subnetwork produces some compact internal representation.
The second, cheaper subnetwork, accepts this internal representation as input, along with the partially produced
output, and produces the next part of the output. Rather than querying a single expensive network `k` times, then, we
query an expensive subnetwork once, and a cheap subnetwork `k` times.

The Epinet ([Osband et al, 2023](https://arxiv.org/pdf/2107.08924.pdf)) has an architecture like this, although
their motivation is different from ours.

### Legal Move Mask

In Scrabble, it may be difficult for the network to learn the set of legal moves in a given position. Doing this
perfectly demands memorizing the entire lexicon, and it is unlikely the network can learn this through self-play,
given that many words are unlikely to come up even once over the course of, say, 1 billion self-play games.

We can consider helping the network along by first computing a mask of legal (sub-)moves, and then passing
that mask as part of the input. The network should quickly learn that it should only place output mass on
(sub-)moves where the mask is 1, and that moves where the mask is 1 should be considered when evaluating the
value of the current position.

The [GADDAG](https://en.wikipedia.org/wiki/GADDAG) data structure supports efficient calculation of a mask of all legal
moves given a board and a rack. We need to compute GADDAG masks anyways to mask network policy outputs,
so passing the masks as a network input should not represent a significant extra cost.

### Leaf Rollouts

In standard AlphaZero MCTS, when you reach a leaf node `n`, you obtain a V estimate from the network, and backpropagate
that estimate up the tree. We can instead do a shallow rollout from `n`, down to some descendant `d`, and then
instead generate our estimate at `d` and backpropagate that estimate to `n` (and `n`'s ancestors). The descent
from `n` to `d` can be done by sampling from `P` (and `H` and the bag) at each step. This generalizes Quackle's
Monte Carlo sampling mechanics.

Without these leaf rollouts, we may be overly sensitive to inaccuracies in V. In general, for V to be perfect,
the network requires a representation of the entire lexicon, which runs into the problems mentioned above. If
the current/projected game state involves rare words that the network's implicit internal lexicon does not yet know,
V could be deficient. Rollouts help us smoothen out this problem, since the moves along the rollout will be appropriately
informed by GADDAG-calculated move masks.

A downside of these leaf rollouts is that the randomness of the `H`-sampling and the bag-replenishing from `n` to `d`
can introduce noise to the leaf evaluation. This can be mitigated by _MCTS Backpropagation Denoising_,
a new idea of ours that is described [here](https://github.com/shindavid/AlphaZeroArcade/blob/main/docs/AuxValueTarget.md#idea-3-stochastic-alphazero-only-mcts-backpropagation-denoising).

### Shared Network Representation

Ideally, P, V, and H are all separate heads of one neural network, so that they benefit from a shared internal
representation. It is unclear how to support this sort of architecture efficiently, given the planned chunk-wise
sampling of H. Do we simply produce P and V outputs each time we sample H, and just discard them?

For now, the plan is to have H as a separate network. We can explore different architectures in the future.
