# Scrabble

This document sketches out a plan for tackling the game of Scrabble.

## Background

Our goal is to create the first superhuman (by 2024 standards) Scrabble AI.

Quackle is an open source program considered to be one of the strongest in the world. It works as follows:

- From any given board state and rack, every candidate move is enumerated, and that move’s _equity_ is computed.
Equity is `(A + B)`, where `A` is the move’s score value, and where `B` is an estimate of the value of the
move’s leave (the tiles remaining on the rack after the move is played).
- For the top-k equity moves, a Monte Carlo simulation is performed, traversing a shallow depth down the game tree.
Each traversal samples the opponent’s rack uniformly randomly from the unseen tiles, and picks the top-equity move
for each player. Of the k tested moves, the one that performs best is chosen.

The leave-estimator is a simple lookup table for every possible leave, generated by some sort of offline simulation process.

In the endgame, Scrabble is a game of perfect information, like chess. As such, Quackle switches to a different mode, both
for the endgame, and for the pre-endgame. We will omit discussion of these parts, as perfect information games are
better understood.

Grandmaster Kenji Matsumoto has detailed the weaknesses of Quackle on this [page](http://www.breakingthegame.net/computers5). Summarizing his observations:

- The AI has poor long-term planning, not willing to hold strong tiles for multiple turns.
- The uniform random assumption of the opponent’s rack is naive. In reality an opponent’s play gives us important information about their likely remaining tiles.
- Equity estimation is ignorant of board dynamics.
- Equity maximization fails to navigate the expectation-vs-variance tradeoff.

A principled, game-theoretically-sound approach, that converges towards the game’s Nash Equilibrium, should naturally overcome all these shortcomings.

## Tree Search in Imperfect Information Games

A common objection to applying a tree-search algorithm to games of imperfect information
is that in imperfect information games, you cannot analyze a subtree of the game in isolation. 
This is because the value of a node in the game tree is dependent on the policy that got to that point in the tree.

Well...that’s only _partially_ true. It is true that the worst-case value of the node against the set 
of _all possible_ adversary policies is dependent on the policy that got to that point in the tree. 
However, against any given _fixed_ adversary policy, the value is independent of this previous policy.
So, if we hold the adversary policy fixed, a tree-search subroutine like MCTS could still act as a
policy improvement operator.

This begs the question: what good is policy improvement against some given _fixed_ policy? In rock-paper-scissor,
against a fixed 100%-rock opponent, iterated policy improvement will lead you to the 100%-paper strategy. That is
not the policy you want to converge to if you want to do well against other policies.

Despite this cautionary example, the fact is, other proven approaches like CFR and R-NaD operate similarly:
on any given generation, they assume a fixed adversary policy and perform a policy-improvement operator with
respect to that _fixed_ policy. Doing so yields a _sequence_ of policy-pairs, and as the policies in the pair
iteratively improve against each other, this _sequence_ (provably) converges towards equilibrium.

So, can MCTS, in the AlphaZero context, similarly serve as a policy-improvement operator that, despite
only representing an improvement against a fixed-opponent-policy _within_ a generation, represents a
long-term improvement towards equilibrium _across_ generations?

## Information Set MCTS (ISMCTS)

In perfect information settings, each node of an MCTS tree represents a full game state.
In imperfect information settings, each node instead presents an _information set_. This is the
part of the game state that is visible to the acting player.

Obtaining a policy (P) and value (V) estimate at the root of the tree, when we are the current
acting player, is straightforward: train a neural network that takes as input the public game state together
with our private information. The big question is how to handle non-root nodes. Recursing at the next
level of the tree is not straightforward, because we are missing the private information of our opponent.
Without that, we cannot construct a proper input to pass to the network, and so cannot obtain a P and V estimate.

We thus need to instantiate the private information of our opponent. In prior approaches, such as
Multiple-Observer Information Set MCTS (MO-ISMCTS), introduced by Cowling et al in 2012
([paper](https://eprints.whiterose.ac.uk/75048/1/CowlingPowleyWhitehouse2012.pdf)), this instantiation was
performed by sampling uniformly randomly, which is not sound. Instead, we will train a _hidden-state_ neural network (H)
that learns to sample the hidden state of the game (the opponent's rack in Scrabble). H will accept the same inputs
as P and V, and will be trained on self-play games using the actual racks as training targets.

There are `>1e7` possible racks in Scrabble, and outputting a logit distribution over all of them is likely unwieldy. 
Instead, we can have H sample the rack `t` tiles at a time, which only requires an output logic layer of size `27^t`.
We can sample the entire 7-tile rack by performing `ceil(7/t)` queries to H, including the partially produced rack
as part of the input. In general games, an appropriate representation of the game's hidden state should be chosen
that allows for a tractable neural network output layer representation.

Now, the proper way to do the descent, selection, and backpropagation steps in the ISMCTS setting is a bit tricky.
The MO-ISMCTS algorithm described by Cowling et al is actually not sound (see [here](MO_ISMCTS_soundness.md) for details).

Briefly, we require parallel trees for each possible point-of-view (POV), and hidden information should not leak
across different POV's. TODO: describe more details
