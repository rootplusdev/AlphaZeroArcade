# Scrabble

This document sketches out a plan for tackling the game of Scrabble.

## Background

Our goal is to create the first superhuman (by 2024 standards) Scrabble AI.

Quackle is an open source program considered to be one of the strongest in the world. It works as follows:

- From any given board state and rack, every candidate move is enumerated, and that move’s _equity_ is computed.
Equity is `(A + B)`, where `A` is the move’s score value, and where `B` is an estimate of the value of the
move’s leave (the tiles remaining on the rack after the move is played).
- For the top-k equity moves, a Monte Carlo simulation is performed, traversing a shallow depth down the game tree.
Each traversal samples the opponent’s rack uniformly randomly from the unseen tiles, and picks the top-equity move
for each player. Of the k tested moves, the one that performs best is chosen, measured by score-differential + equity-differential.

The leave-estimator is a simple lookup table for every possible leave, generated by some sort of offline simulation process.

In the endgame, Scrabble is a game of perfect information, like chess. As such, Quackle switches to a different mode, both
for the endgame, and for the pre-endgame. We will omit discussion of these parts, as perfect information games are
better understood.

Grandmaster Kenji Matsumoto has detailed the weaknesses of Quackle on this [page](http://www.breakingthegame.net/computers5). Summarizing his observations:

- The AI has poor long-term planning, not willing to hold strong tiles for multiple turns.
- The uniform random assumption of the opponent’s rack is naive. In reality an opponent’s play gives us important information about their likely remaining tiles.
- Equity estimation is ignorant of board dynamics.
- Equity maximization fails to navigate the expectation-vs-variance tradeoff.

A principled, game-theoretically-sound approach, that converges towards the game’s Nash Equilibrium, should naturally overcome all these shortcomings.

## Tree Search in Imperfect Information Games

A common objection to applying a tree-search algorithm to games of imperfect information
is that in imperfect information games, you cannot analyze a subtree of the game in isolation. 
This is because the value of a node in the game tree is dependent on the policy that got to that point in the tree.

Well...that’s only _partially_ true. It is true that the worst-case value of the node against the set 
of _all possible_ adversary policies is dependent on the policy that got to that point in the tree. 
However, against any given _fixed_ adversary policy, the value is independent of this previous policy.
So, if we hold the adversary policy fixed, a tree-search subroutine like MCTS could still act as a
policy improvement operator.

This begs the question: what good is policy improvement against some given _fixed_ policy? In rock-paper-scissor,
against a fixed 100%-rock opponent, iterated policy improvement will lead you to the 100%-paper strategy. That is
not the policy you want to converge to if you want to do well against other policies.

Despite this cautionary example, the fact is, other proven approaches like CFR and R-NaD operate similarly:
on any given generation, they assume a fixed adversary policy and perform a policy-improvement operator with
respect to that _fixed_ policy. Doing so yields a _sequence_ of policy-pairs, and as the policies in the pair
iteratively improve against each other, this _sequence_ (provably) converges towards equilibrium.

So, can MCTS, in the AlphaZero context, similarly serve as a policy-improvement operator that, despite
only representing an improvement against a fixed-opponent-policy _within_ a generation, represents a
long-term improvement towards equilibrium _across_ generations?

## Information Set MCTS (ISMCTS)

In perfect information settings, each node of an MCTS tree represents a full game state.
In imperfect information settings, each node instead presents an _information set_. This is the
part of the game state that is visible to the acting player.

Obtaining a policy (P) and value (V) estimate at the root of the tree, when we are the current
acting player, is straightforward: train a neural network that takes as input the public game state together
with our private information. The big question is how to handle non-root nodes. Recursing at the next
level of the tree is not straightforward, because we are missing the private information of our opponent.
Without that, we cannot construct a proper input to pass to the network, and so cannot obtain a P and V estimate.

We thus need to instantiate the private information of our opponent. In prior approaches, such as
Multiple-Observer Information Set MCTS (MO-ISMCTS), introduced by Cowling et al in 2012
([paper](https://eprints.whiterose.ac.uk/75048/1/CowlingPowleyWhitehouse2012.pdf)), this instantiation was
performed by sampling uniformly randomly, which is not sound. Instead, we will train a _hidden-state_ neural network (H)
that learns to sample the hidden state of the game (the opponent's rack in Scrabble). H will accept the same inputs
as P and V, and will be trained on self-play games using the actual racks as training targets.

There are `>1e7` possible racks in Scrabble, and outputting a logit distribution over all of them is likely unwieldy. 
Instead, we can have H sample the rack `t` tiles at a time, which only requires an output logic layer of size `27^t`.
We can sample the entire 7-tile rack by performing `ceil(7/t)` queries to H, including the partially produced rack
as part of the input. In general games, an appropriate representation of the game's hidden state should be chosen
that allows for a tractable neural network output layer representation.

Now, the proper way to do the descent, selection, and backpropagation steps in the ISMCTS setting is a bit tricky.
The MO-ISMCTS algorithm described by Cowling et al is actually not sound (see [here](MO_ISMCTS_soundness.md) for details).
Briefly, we require parallel trees for each possible point-of-view (POV), and hidden information should not leak
across different POV's. TODO: describe more details

## Convergence to Equilibrium

If we play self-play games using ISMCTS with `n` visits, and train P, V and H on the complete resultant set of 
self-play data, can we expect convergence to Nash Equilibrium, as `n` approaches infinity?

Formally, if we imagine the combined weights of the P, V, and H networks to be a point in `R^d`, then the generations
of training yields a path-like sequence of points in `R^d`: `x_1, x_2, ...`. The game's Nash equilibrium is some subset `NE` of `R^d`.
Does `x_i` have a limit, and if so, is that limit in `NE`?

Here is a soft-proof that the answer is yes.

The proof entails 3 parts:

1. **(Idempotence)**: If currently at equilibrium, we will stay there.
2. **(Non-Complacency)**: If not currently at equilibrium, we cannot stay there.
3. **(Limit-existence)**: The limit of `x_i` must exist.

These 3 assertions clearly prove our desired result. Let us prove them in turn.

### Idempotence

Suppose that P, V, and H are at equilibrium.

The PUCT formula is:

```
PUCT(a) = Q(a) + c * P(a) * sqrt(sum(N)) / (1 + N(a))
```

The idempotence requirement means that as the number of iterations approaches infinity, the distribution of `N`
should approach the same shape as `P`. 

By standard properties of Nash equilibria, the assumption of equilibrium translates to the following: 
`P(a) == 0` for all a not in `S`, where `S` is the set of actions `a` for which `V(a)` attains it maximum
of `v_max`.

If `V` is accurate, as presumbed by the equilibrium hypothesis, then `Q(a)` at actions in `S` should converge
towards `v_max`, while `Q(a)` at actions not in `S` should converge towards values less than `v_max`. Since
`Q` dominates the equation as `N(a)` approach infinity, the proportion of `N` on actions not in `S`
should approach 0. It remains to show that the ratio `N(a_i) / N(a_j)` approaches `P(a_i) / P(a_j)`
for all `a_i, a_j` in `S`.

To see this, we can plug `a_i` and `a_j` into the `PUCT` equation, and set PUCT values equal. The `Q(a)`, 
`c`, and `sqrt(sum(N))` terms all cancel, leaving us with:

```
P(a_i) / P(a_j) = (1 + N(a_i)) / (1 + N(a_j))
```

This is enough. We can make the convergence better by replacing the `(1 + N(a))` term in the PUCT formula with
`max(1, N(a))`.

Note that it is important that we disable root softmax temperature for this to proof to work.

### Non-Complacency

Suppose P, V and H are not at equilibrium. Then, there are game states at which they produce non-equilibrium estimates.
ISMCTS should then produce a policy at one or more nodes that exploits these misestimates, and the resultant
self-play data should cause a drift in `R^d`.

### Limit-existence

If the limit does not exist, then the `x_i` necessarily follow a cyclical path. If we train on the full self-play
dataset, however, an infinite cyclical path is not possible; we must eventually fall into the path's interior.
