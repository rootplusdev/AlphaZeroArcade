"""
An important consideration in training is how to sample data. We have an ever-growing sequence of
positions generated by self-play, and at each generation, we want to do a training epoch by sampling
some subset of the current sequence. Some high-level goals:

- We don't want to sample any row too many times, since this can lead to overfitting.

- We want approximate reproducibility: if we train on self-play-data as it arrives and get to
  generation N, we want to be able to train a new model from scratch on that same set of
  self-play-data for N epochs and arrive at approximately the same model weights.

My methodology to meet these goals is based on KataGo and LeelaZeroChess:

- The self-play process produces a chronologically-sorted master list M of positions, which
  continually grows.

- A generation G, we choose a length-N_G contiguous subsequence S of M, and sample R positions
  from S, where R is a constant. We train on those R positions, and mark that each position in S
  were sampled R/N_G times in expectation.

- We support two different policies for selecting S:

  1. Prefix-based: S consists of the first N_G positions in M.
  2. Suffix-based: S consists of the last N_G positions in M.

- After doing this, if any position was sampled more than K times in expectation, we discard that
  position and all positions preceding it.

- If there are not N_G positions available, then we wait for more data to be generated.

The prefix-based policy leads to greater reproducibility, as it trains on the same subsequences of
the master list regardless of whether the master list is constructed up-front or in streaming
fashion. It is also data-efficient, in the sense that every position ends up being used at least
K times in expectation. However, compared to the suffix-based policy, it can take longer for a
given recent position to be processed P times in expectation, for any P. The training process can
sit idly, waiting for new data, while a recent position has barely been processed.

The suffix-based policy leads to less reproducibility, as it trains on different subsequences of
the master list depending on the relative timing of the training process and the self-play process.
However, in the standard case where the self-play process is the bottleneck, it should behave in a
relatively predictable manner. Compared to the prefix-based policy, the training process is more
likely to fully utilize the most recent data, which can make it a more effective companion of a live
self-play process.

Currently, during live AlphaZero runs, we use...

- the suffix-based policy by default during live AlphaZero runs
- KataGo's formula for N_G, which is a shifted/scaled version of the function f(n) = n^0.75,
  where n = |M|.

For offline controlled experiments on already-produced self-play data, however, we use the
prefix-based policy with an N_G value that is independent of |M|. This allows for better
apples-to-apples comparisons across different experiments.
"""
from typing import List, Optional

import torch
from torch.utils.data import Dataset

from alphazero.data.metadata import SelfPlayMetadata
from util.infinite_sequence import InfiniteSequence
from util.torch_util import Shape


class GamesDataset(Dataset):
    """
    A GamesDataset represents a slice of a master list of game-positions.
    """
    def __init__(self, self_play_metadata: SelfPlayMetadata, start: int, end: int):
        """
        self_play_metadata: effectively represents a master list (M) of positions

        Constructs a GamesDataset corresponding to M[start:end]
        """
        assert 0 <= start < end, (start, end)
        self.self_play_metadata = self_play_metadata
        self.n_total_positions = self_play_metadata.n_total_positions
        self.start = start
        self.end = end
        self.window = list(self.self_play_metadata.get_window(start, end))
        self.key_order: List[str] = []

    def __str__(self):
        first_gen = self.window[0].game_metadata.generation
        last_gen = self.window[-1].game_metadata.generation
        return f'GamesDataset(start={self.start}[gen-{first_gen}], end={self.end}[gen-{last_gen}])'

    def get_input_shape(self) -> Shape:
        """
        Peeks into the dataset to determine the shape of the input.

        Without this, we would need to hard-code the input shape into the model configuration,
        which could be cumbersome if we want to experiment with different ways of representing the
        input. For example, we may want to vary the number of previous positions of history we
        include in the input, or we may want to add some additional input feature planes.
        """
        position_metadata = self.window[0]
        game_metadata = position_metadata.game_metadata
        data = torch.jit.load(game_metadata.filename).state_dict()
        return data['input'].shape[1:]

    def get_target_names(self) -> List[str]:
        """
        Peeks into the dataset to find the names of all the targets.
        """
        position_metadata = self.window[0]
        game_metadata = position_metadata.game_metadata
        data = torch.jit.load(game_metadata.filename).state_dict()
        names = list(data.keys())
        return [n for n in names if n != 'input']

    def set_key_order(self, target_names: List[str]):
        """
        The key order determines the order in which the data is returned by __getitem__.

        This must be called prior to iterating over the dataset.
        """
        self.key_order = ['input'] + target_names

    def __len__(self):
        return self.end - self.start

    def __getitem__(self, idx):
        assert self.key_order, 'Must call set_key_order() before iterating over GamesDataset'
        position_metadata = self.window[idx]
        game_metadata = position_metadata.game_metadata
        p = position_metadata.position_index
        try:
            data = torch.jit.load(game_metadata.filename).state_dict()
        except:
            raise Exception('Could not load data from file: {}'.format(game_metadata.filename))
        return [data[key][p] for key in self.key_order]


class GamesDatasetGenerator:
    """
    Example usage:

    sample_limit = 10
    use_prefix = True
    generator = GamesDatasetGenerator(self_play_data_dir, sample_limit)
    while True:
        dataset = generator.get_next_dataset(loader_size, use_prefix)
        if dataset is None:
            # wait for more data to be generated
            time.sleep(5)
            continue

        ...  # do an epoch network training here by iterating over dataset

        generator.record_dataset_usage(dataset, num_positions_sampled)

    In the above, self_play_data_dir is a directory that contains a master list of positions. A
    GamesDatasetGenerator effectively maintains a list M that is initially equal to this master
    list.

    On every loop iteration, the generator extends M with any newly generated data, and returns a
    GamesDataset corresponding to a subsequence of M.

    When record_dataset_usage() is called, the generator marks that num_positions_sampled positions
    were uniformly randomly sampled from dataset. It then identifies any positions in M that were
    sampled >= sampled_limit times, and effectively discards those positions and all positions
    preceding them from M.
    """
    def __init__(self, self_play_data_dir: str, sample_limit: Optional[int] = None):
        """
        sample_limit dictates the maximum expected number of times any given training row can be
        used by the training process. If None, then there is no limit.

        KataGo uses a limit of 4. LeelaChessZero effectively uses a limit of 8.
        """
        self.expected_sample_counts = InfiniteSequence()
        self.self_play_metadata = SelfPlayMetadata(self_play_data_dir)
        self.self_play_data_dir = self_play_data_dir
        self.sample_limit = sample_limit
        self.head_index = 0

    @property
    def n_total_positions(self) -> int:
        return self.self_play_metadata.n_total_positions

    def get_next_dataset(self, loader_size: int, use_prefix: bool,
                         verbose: bool = False) -> Optional[GamesDataset]:
        """
        Returns a DataLoader corresponding to a slice of the master list of size loader_size. If
        use_prefix is True, the slice is taken from the beginning of the master list. Otherwise, it
        is taken from the end.

        If there are not enough positions available, returns None. If it returns None, it is
        recommended to sleep for a few seconds before trying again, to avoid thrashing the
        filesystem.
        """
        if use_prefix:
            assert self.sample_limit is not None, 'use_prefix=True requires sample_limit to be set'

        self.self_play_metadata.refresh()
        n_total_positions = self.self_play_metadata.n_total_positions

        available_positions = n_total_positions - self.head_index
        if available_positions < loader_size:
            return None

        if use_prefix:
            start = self.head_index
            end = start + loader_size
        else:
            end = n_total_positions
            start = end - loader_size

        dataset = GamesDataset(self.self_play_metadata, start, end)
        if verbose:
            print('Total positions:', n_total_positions)
            print('head_index:', self.head_index)
            print('expected_sampled_counts:\n ',
                  self.expected_sample_counts.to_string(delim='\n  ', cap=self.sample_limit))
            print('Dataset:', dataset)

            start_pct = 100. * start / n_total_positions
            end_pct = 100. * end / n_total_positions
            print('Data range: [%.3f%% - %.3f%%]' % (start_pct, end_pct))
        return dataset

    def record_dataset_usage(self, dataset: GamesDataset, num_positions_sampled: int):
        """
        Marks that num_positions_sampled positions were uniformly randomly sampled from dataset.
        It then identifies any positions in M that were sampled >= sampled_limit times, and
        effectively discards those positions and all positions preceding them from M.
        """
        start = dataset.start
        end = dataset.end
        x = num_positions_sampled / (end - start)
        self.expected_sample_counts[start:end] += x
        self.head_index = self.expected_sample_counts.get_start(self.sample_limit)

        # optional step to shrink expected_sample_counts:
        if False:
            self.expected_sample_counts[:self.head_index] = self.sample_limit + 1


def get_katago_sample_size(n_total: int, alpha=0.75, beta=0.4, c=250000) -> int:
    """
    Returns the number of positions from which to sample, as a function of the total number of
    positions in the master list.

    This is the sublinear curve f(n) = n^alpha but rescaled so that f(c) = c and f'(c) = beta.

    From Appendix C of KataGo paper.

    https://arxiv.org/pdf/1902.10565.pdf
    """
    return min(n_total, int(c * (1 + beta * ((n_total / c) ** alpha - 1) / alpha)))
